#### 启动集群

第一次要先格式化102的namenode

```
hdfs namenode -format
```

在102启动hdfs

```
start-dfs.sh
```

在103启动yarn

```
start-yarn.sh
```

配置同步

```
 xsync /opt/module/hadoop-3.3.4/etc/hadoop/mapred-site.xml
```

hadoop常用命令（hadoop fs - linux命令）

```
[had@hadoop102 ~]$ hadoop fs -ls /
Found 6 items
drwxr-xr-x   - had supergroup          0 2025-01-06 01:52 /c01
drwxr-xr-x   - had supergroup          0 2025-01-06 00:29 /input
drwx------   - had supergroup          0 2025-01-06 01:15 /tmp
drwxr-xr-x   - had supergroup          0 2025-01-06 00:31 /wcoutput
drwxr-xr-x   - had supergroup          0 2025-01-06 00:46 /wcoutput2
drwxr-xr-x   - had supergroup          0 2025-01-06 01:15 /wcoutput3
```

创建目录

```
[had@hadoop102 ~]$ hadoop fs -mkdir /c01
```

剪切到hdfs

```
[had@hadoop102 myfiles]$ cd wcinput/
[had@hadoop102 wcinput]$ ll
总用量 4
-rw-rw-r--. 1 had had 45 1月   5 22:29 word.txt
[had@hadoop102 wcinput]$ echo "tom,jack,rose" >> stus.txt
[had@hadoop102 wcinput]$ hadoop fs -moveFromLocal stus.txt /c01
```

拷贝到hdfs（-copyFromLocal和-put等价）

```
[had@hadoop102 myfiles]$ echo "abcdefgh" >>stus2.txt 
[had@hadoop102 myfiles]$ hadoop fs -copyFromLocal stus2.txt /c01
```

将文件追加到末尾

```
[had@hadoop102 myfiles]$ hadoop fs -appendToFile stus2.txt /c01/stus.txt
```

文件下载

```
[had@hadoop102 myfiles]$ hadoop fs -copyToLocal /c01/stus.txt
[had@hadoop102 myfiles]$ ll
总用量 8
-rw-rw-r--. 1 had had  9 1月   6 04:49 stus2.txt
-rw-r--r--. 1 had had 23 1月   6 04:57 stus.txt
drwxrwxr-x. 2 had had 22 1月   6 04:26 wcinput
[had@hadoop102 myfiles]$ cat stus.txt 
tom,jack,rose
abcdefgh
```

chmod

```
[had@hadoop102 myfiles]$ hadoop fs -chmod 777 /c01/stus.txt
```

chown

```
[had@hadoop102 myfiles]$ hadoop fs -chown had:had /c01/stus.txt
```

du

```
[had@hadoop102 myfiles]$ hadoop fs -du -s -h /c01
32  96  /c01
```

setrep（设置副本数量）

```
[had@hadoop102 myfiles]$  hadoop fs -setrep 2 /c01/stus.txt
Replication 2 set: /c01/stus.txt
[had@hadoop102 myfiles]$ hadoop fs -du  -h /c01
23  46  /c01/stus.txt
9   27  /c01/stus2.txt
```

