​	

### OutputFormat体系

是Mapreduce所有输出类的基类

OutputFormat是个抽象类有三种重要的抽象方法

- RecordWriter（）返回一个RecordWriter类对象，负责将键值对写出 	ecs（）检查当前job输出是否规范，检查目录是否存在
- getOutputCommitter（）负责job的输出环境

只要继承OutputFormat类就要重写三个抽象方法

```java
@InterfaceAudience.Public
@InterfaceStability.Stable
public abstract class OutputFormat<K, V> {

  /*
    获取给定任务的 {@link RecordWriter}。
    @param context 当前任务的信息。
    @return 一个 {@link RecordWriter} 用于写入作业的输出。
    @throws IOException 如果发生 I/O 错误 
   */
  public abstract RecordWriter<K, V> 
    getRecordWriter(TaskAttemptContext context
                    ) throws IOException, InterruptedException;

  /*
    检查作业的输出规范的有效性。
    <p>这是用于在提交作业时验证输出规范。通常检查输出是否已存在，
    如果已经存在则抛出异常，以防止输出被覆盖。</p>
    支持委托令牌的文件系统的实现通常会收集目标路径的令牌，
    并将它们附加到作业上下文的 JobConf 中。
    @param context 关于作业的信息
	@throws IOException 如果不应该尝试输出，则抛出此异常
  */
  public abstract void checkOutputSpecs(JobContext context
                                        ) throws IOException, 
                                                 InterruptedException;

   /*
    获取此输出格式的输出提交者。它负责确保输出正确提交。
    @param context 任务上下文
    @return 输出提交者
    @throws IOException 如果发生 I/O 错误
    @throws InterruptedException 如果线程被中断 
   */
  public abstract 
  OutputCommitter getOutputCommitter(TaskAttemptContext context
                                     ) throws IOException, InterruptedException;
}
```

### 自定义OutputFormat



#### StudentBean

```java
import lombok.Getter;
import lombok.Setter;
import org.apache.hadoop.io.Writable;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

@Getter
@Setter
public class StudentBean  implements Writable {
//    202101    孙悟空    语文 100
    private int stuId;
    private String name;
    private String subject;
    private double score;

    public StudentBean(){}

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeInt(this.stuId);
        out.writeUTF(name);
        out.writeUTF(subject);
        out.writeDouble(this.score);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.stuId = in.readInt();
        this.name=in.readUTF();
        this.subject=in.readUTF();
        this.score=in.readDouble();
    }

    @Override
    public String toString() {
        return +this.stuId+"\t"+this.name+"\t"+this.subject+"\t"+this.score;
    }
}
```

#### StudentMapper

```java
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class StudentMapper extends Mapper<LongWritable, Text,Text, StudentBean> {

    private Text outK = new Text();
    private  StudentBean outV =new StudentBean();
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, StudentBean>.Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split("\t");
        //    202101    孙悟空    语文 100
        String stuId = arr[0];
        String name = arr[1];
        String subject = arr[2];
        String score = arr[3];

        outK.set(stuId);

        outV.setStuId(Integer.valueOf(stuId));
        outV.setName(name);
        outV.setSubject(subject);
        outV.setScore(Double.valueOf(score));

        context.write(outK,outV);
    }
}
```

#### StudentReducer

```java
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class StudentReducer extends Reducer<Text,StudentBean,Text,StudentBean> {
    @Override
    protected void reduce(Text key, Iterable<StudentBean> values, Reducer<Text, StudentBean, Text, StudentBean>.Context context) throws IOException, InterruptedException {
        for(StudentBean studentBean : values){
            context.write(key,studentBean);
        }
    }
}
```

#### StudentOutputFormat

```java
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


import java.io.IOException;

public class StudentOutputFormat extends FileOutputFormat<Text,StudentBean> {
//    接收到的是reduce的输出
    @Override
    public RecordWriter<Text, StudentBean> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {
        StudentRecordWriter studentRecordWriter = new StudentRecordWriter(job);
        return studentRecordWriter;

    }
}
```

#### StudentRecordWriter

```java
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;

import java.io.IOException;

public class StudentRecordWriter extends RecordWriter<Text,StudentBean> {
    FSDataOutputStream outLow;
    FSDataOutputStream outHigh;

    public StudentRecordWriter(TaskAttemptContext job)  {
        try {
//            获取Hadoop配置并创建一个文件系统对象
            FileSystem fs = FileSystem.get(job.getConfiguration());

            outLow = fs.create(new Path("E:/xiangmu/Hadoopc01/file/outputs/day0117/aaaa.txt"));
            outHigh = fs.create(new Path("E:/xiangmu/Hadoopc01/file/outputs/day0117/bbbb.txt"));

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @Override
    public void write(Text key, StudentBean studentBean) throws IOException, InterruptedException {
        String line = studentBean.getStuId()+"\t"+studentBean.getName()+"\t"+studentBean.getSubject()+"\t"+studentBean.getScore();
        line=line+"\n";

        if ((studentBean.getStuId()+"").startsWith("2021")||(studentBean.getStuId()+"").startsWith("2022")){
            outLow.write(line.getBytes());
        }else {
            outHigh.write(line.getBytes());
        }
    }

    @Override
    public void close(TaskAttemptContext context) throws IOException, InterruptedException {
        IOUtils.closeStream(outLow);
        IOUtils.closeStream(outHigh);
    }
}
```

#### StudentDriver

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class StudentDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        Configuration configuration=new Configuration();
        Job job =Job.getInstance(configuration);

        job.setJarByClass(StudentDriver.class);

        job.setMapperClass(StudentMapper.class);
        job.setReducerClass(StudentReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(StudentBean.class);

        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(Text.class);

        job.setOutputFormatClass(StudentOutputFormat.class);

        FileInputFormat.setInputPaths(job,new Path("E:/xiangmu/Hadoopc01/file/input/day0117/outputFormat"));
        FileOutputFormat.setOutputPath(job,new Path("E:/xiangmu/Hadoopc01/file/output/day0117/outputFormat"));

        boolean b =job.waitForCompletion(true);
        System.exit(b ? 0 : 1);

    }
}
```





## MR的join

 

















































































